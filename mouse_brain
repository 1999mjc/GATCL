import torch
import pandas as pd
import numpy as np
import h5py
import anndata as ad
import scanpy as sc
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from build_graph import *
from model import *

# --- Import necessary evaluation metrics (Assumed to be installed) ---
from sklearn.metrics import (
    homogeneity_score, mutual_info_score, v_measure_score,
    adjusted_mutual_info_score, normalized_mutual_info_score, adjusted_rand_score
)

def compute_scores(true, pred):
    """Calculates all supervised clustering evaluation metrics."""
    return {
        "Homogeneity": homogeneity_score(true, pred),
        "Mutual_info": mutual_info_score(true, pred),
        "V_measure": v_measure_score(true, pred),
        "AMI": adjusted_mutual_info_score(true, pred),
        "NMI": normalized_mutual_info_score(true, pred),
        "ARI": adjusted_rand_score(true, pred)
    }
    
def run_example():
    """Runs the full GATCL workflow: data loading, preprocessing, training, clustering, and metric calculation."""
    
    #  example data
    RNA_DATA_PATH = "./data/mouse_spleen_rna" # Path to the RNA data file
    PROT_DATA_PATH = "./data/mouse_spleen_atac" # Path to the Protein/ATAC data file
    K_NEIGHBORS = N # K-Nearest Neighbors used for graph construction
    #TARGET_CLUSTERS = 7 # Target number of clusters
    # Load Data
    try:
        adata_rna = sc.read_h5ad(RNA_DATA_PATH)
        adata_prot = sc.read_h5ad(PROT_DATA_PATH)
    except FileNotFoundError:
        print("\nERROR: Data files not found. Please check paths.")
        return
    
    # 5. Apply PCA for final feature generation 
    PCA_COMPONENTS = 100  ##choose the proper number
    sc.tl.pca(adata_rna, n_comps=100)
    sc.tl.pca(adata_prot, n_comps=100)
    
    # 6. Set final feature representations in obsm['feat'] for model input
    adata_rna.obsm['feat'] = adata_rna.obsm["X_pca"]
    adata_prot.obsm['feat'] = adata_prot.obsm["X_pca"]
    
    # --- Check for Necessary Features (after preprocessing) ---
    if 'feat' not in adata_rna.obsm or 'spatial' not in adata_rna.obsm:
        print("\nERROR: AnnData objects must contain 'feat' (embeddings/PCA) and 'spatial' (coordinates) in .obsm")
        print("Please ensure spatial coordinates are available.")
        return

    N_SPOTS = adata_rna.n_obs
    print(f"Data Loaded: {N_SPOTS} Spots.")
    
    # 2. Graph Construction
    data_pkg = build_graphs(adata_rna, adata_prot, n_neighbors=K_NEIGHBORS)
    
    # 3. Model Instantiation and Training (Assumes GATCL_Trainer is defined)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU if available
    trainer = GATCL_Trainer(
        data=data_pkg,
        device=device
    )
    
    # 4. Execute Training, get final embedding
    output = trainer.train()
    final_embedding = output['embedding']
    
    # 5. Save embedding result
    np.save("combine_embedding.npy", final_embedding)
    np.save(" alpha_1.npy", output['alpha_1'])
    np.save(" alpha_2.npy", output['alpha_2'])
    np.save(" alpha_cross.npy", output['alpha_cross'])
    np.savetxt("embedding.csv", embedding, delimiter=",",  fmt='%.6f')
    ########################################################
    # --- Clustering(RStudio)
    # Define the path to the input data file.
    data_file <- "embedding.csv"

     # 2. Install and load necessary R packages (if not already installed)
     # Check if the 'mclust' package is installed.
     if (!requireNamespace("mclust", quietly = TRUE)) {
      # If not installed, install the package.
        install.packages("mclust")
       }
     # Load the 'mclust' library for model-based clustering.
     library(mclust)

     # 3. Set a random seed to ensure reproducible results
     # Set a specific seed for random number generation.
     set.seed(2020) 

     # 4. Read the data
     # Print a message indicating data reading is in progress.
     cat("reading data...\n")
     # Read the data into an R data frame. header=FALSE indicates no header, sep="," uses comma as delimiter.
     data_for_clustering <- read.csv(data_file, header=FALSE, sep=",")

     # 5. Execute Mclust clustering
     # Define the desired number of clusters (K).
     K <- 5 # choose a value you want 
     # Print a message about the start of clustering, including the value of K.
     cat(paste(" Mclust clusteringï¼ŒK =", K, "...\n"))

     # Execute the clustering process using Mclust (Gaussian Mixture Models).
     # G = K sets the number of components. modelNames = "EEE" specifies an ellipsoidal model with equal shape and equal orientation.
     mclust_result <- Mclust(data_for_clustering, G = K, modelNames = "EEE")

     # 6. Output and save clustering results
     # Print a confirmation message.
     cat("Mclust finishe!\n\n")

     # View summary information (optional, for checking results)
     # Display a summary of the Mclust results.
     summary(mclust_result)

     # Extract the resulting cluster assignments (labels) into a vector.
     cluster_labels <- mclust_result$classification

     # 2. Define the output filename
     # Define the path for the output CSV file.
     csv_output_file <- "mclust_classification_results_R.csv"

     # 1) Convert the cluster labels vector into a single-column data frame.
     data_to_save <- data.frame(cluster_label = cluster_labels)

     # 2) Use write.csv() function to save the data frame to a CSV file.
     # The data frame to be saved.
     write.csv(
     # The output file path.
          data_to_save,
          file = csv_output_file,
        row.names = FALSE
        )
######################################################################
    #######calculate the evaluation metrics
    # Load the predicted cluster labels saved from the R script (or previous step).
    labels = pd.read_csv('mclust_classification_results_R.csv')

    # Load the ground truth annotations from an Excel file.
    annotation = pd.read_excel(
    # Specify the full path to the annotation file.
           "./data/annotiation.xlsx"
    )

    # Extract true labels (ground truth) from the 'manual' column of the annotation DataFrame.
    true_labels2 = annotation['manual']
    # Assign predicted labels (the entire 'labels' DataFrame/single column) to pred_labels2.
    pred_labels2 = labels  # The name corresponds to the column saved previously.
    # Convert the Pandas DataFrame (pred_labels2) to a NumPy array.
    pred_labels2 = pred_labels2.to_numpy()
    # Flatten the NumPy array to ensure it is a 1D vector, which is required by clustering evaluation functions.
    pred_labels2 = pred_labels2.flatten()  # Recommended, returns a copy.
    # Calculate clustering evaluation scores (ARI, NMI, etc.) using the custom function.
    scores_gatcl = compute_scores(true_labels2, pred_labels2)
    # Convert the dictionary of scores into a Pandas DataFrame for easy saving.
    df_scores = pd.DataFrame(list(scores_gatcl.items()), columns=['Metric', 'Score'])

    # Save the calculated evaluation metrics to an Excel file.
    # df_scores.to_excel(
    # # Specify the output path and filename for the results.
    #         "GATCL_result.xlsx",
    #    index=False
    #  )
        
    #####spatial domain picture
    # Read the corresponding AnnData, extracting spot names as the index.
    barcodes = adata_rna.obs_names
    # Add the cluster labels to the AnnData's observation metadata (adata_rna.obs).
    adata_rna.obs["cluster_labels"] = labels.astype(str)  # Convert to string type for easier plotting/handling.

    # Get the spatial coordinates from the AnnData object's .obsm slot.
    coords = adata_rna.obsm["spatial"]

    # Generate the base color list.
    # Determine the number of unique clusters present in the labels.
    num_clusters = len(np.unique(labels))
    # Get a base colormap (tab10 is a common categorical map) with enough colors for the clusters.
    base_cmap = plt.cm.get_cmap('tab10', num_clusters)
    # Generate a list of color codes (RGBA tuples) from the base colormap.
    base_colors = [base_cmap(i) for i in range(num_clusters)]

    # 2. Define custom color order - adjust the index order as needed here
    # Define a custom sequence of indices to rearrange the base colors.
    custom_order = [0,1,2,3,4,5]  # This order can be modified as required.

    # Apply the custom order to get the final list of colors used for plotting.
    # Select and reorder colors based on the custom_order indices.
    custom_colors = [base_colors[i] for i in custom_order]

    # 3. Assign color to each point (match custom colors by cluster label)
    # Map the custom colors to each data point based on its cluster label.
    point_colors = [custom_colors[label] for label in labels]

    # 4. Draw spatial plot (add color legend)
    # Create a new figure object with specified size.
    plt.figure(figsize=(6, 5))
    # Draw the scatter plot using coordinates for position and point_colors for color.
    plt.scatter(coords[:, 0], coords[:, 1], c=point_colors, s=30)
    # Invert the Y-axis (a common convention in spatial transcriptomics data visualization).
    plt.gca().invert_yaxis()
    # Turn off the coordinate axes for a cleaner look.
    plt.axis('off')

  
if __name__ == "__main__":
    # Ensure all auxiliary functions (like GATCL_Trainer, build_graphs, etc.) are imported or defined before running
    run_example()
